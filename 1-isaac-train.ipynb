{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsYL9huYpH4r"
   },
   "source": [
    "## ISAAC â€“ Model Training Pipeline\n",
    "\n",
    "Run `1-train-models.ipynb` to:\n",
    "\n",
    "- Train DeepBind, BPNet, and DeepSEA models on TF binding datasets\n",
    "- Save trained model checkpoints and training metrics\n",
    "- Reproduce the predictive models audited in the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QTjO8D6vpH7U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\demo_user\\anaconda3\\envs\\isaac_clean\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.training.model_factory import build_model\n",
    "from src.training.datasets import DNACNNDataset\n",
    "from src.training.loops import train_epoch_cnn, freeze_model\n",
    "from src.training.utils import set_seed, make_loader\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\"Data_A549\", \"Data_GM12878\", \"Data_Hepg2\"]\n",
    "MODELS   = [\"DeepBind\", \"BPNet\", \"DeepSEA\"]\n",
    "\n",
    "EPOCHS = {\n",
    "    \"DeepBind\": 20,\n",
    "    \"BPNet\": 20,\n",
    "    \"DeepSEA\": 20,\n",
    "}\n",
    "\n",
    "SEEDS = [0, 1, 2, 3, 4]\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data/splits\")\n",
    "RESULTS_DIR = Path(\"results/tf_gene/training\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FULL TRAINING LOOP (used to produce paper results)\n",
    "# ============================================================\n",
    "\n",
    "for dataset in DATASETS:\n",
    "\n",
    "    split_dir = DATA_DIR / dataset\n",
    "    assert split_dir.exists(), f\"Missing split dir: {split_dir}\"\n",
    "\n",
    "    train_path = split_dir / \"train_subsampled.csv\"\n",
    "    assert train_path.exists(), f\"Missing train.csv in {split_dir}\"\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "    print(\"\\n===================================\")\n",
    "    print(f\" DATASET: {dataset}\")\n",
    "    print(\"===================================\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "\n",
    "    input_len = len(train_df[\"sequence_full\"].iloc[0])\n",
    "\n",
    "    for model_name in MODELS:\n",
    "\n",
    "        n_epochs = EPOCHS[model_name]\n",
    "\n",
    "        for seed in SEEDS:\n",
    "\n",
    "            set_seed(seed)\n",
    "\n",
    "            out_dir = RESULTS_DIR / dataset / model_name / f\"seed_{seed}\"\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            model_path = out_dir / \"model.pt\"\n",
    "\n",
    "            if model_path.exists():\n",
    "                print(\n",
    "                    f\"[SKIP] {dataset} | {model_name} | seed={seed} \"\n",
    "                    \"(already trained)\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # Run header\n",
    "            # --------------------------------------------------\n",
    "            print(\n",
    "                f\"\\n[TRAIN] {dataset} | {model_name} | seed={seed} \"\n",
    "                f\"| epochs={n_epochs}\"\n",
    "            )\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # Dataset & loader\n",
    "            # --------------------------------------------------\n",
    "            ds = DNACNNDataset(train_df, view=\"sequence_full\")\n",
    "            loader = make_loader(ds, BATCH_SIZE)\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # Model, optimizer, loss\n",
    "            # --------------------------------------------------\n",
    "            model = build_model(\n",
    "                model_name=model_name,\n",
    "                input_length=input_len,\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            opt  = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "            crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            losses = []\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # Training epochs\n",
    "            # --------------------------------------------------\n",
    "            for ep in tqdm(\n",
    "                range(n_epochs),\n",
    "                desc=f\"{model_name} | seed={seed}\",\n",
    "                leave=False,\n",
    "            ):\n",
    "                loss = train_epoch_cnn(\n",
    "                    model, loader, opt, crit, DEVICE\n",
    "                )\n",
    "                losses.append(loss)\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # Freeze & save\n",
    "            # --------------------------------------------------\n",
    "            freeze_model(model)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # Save metrics\n",
    "            # --------------------------------------------------\n",
    "            metrics = {\n",
    "                \"dataset\": dataset,\n",
    "                \"model\": model_name,\n",
    "                \"seed\": seed,\n",
    "                \"epochs\": n_epochs,\n",
    "                \"train_size\": len(train_df),\n",
    "                \"final_train_loss\": losses[-1],\n",
    "                \"train_loss_curve\": losses,\n",
    "            }\n",
    "\n",
    "            with open(out_dir / \"metrics.json\", \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "\n",
    "            print(\n",
    "                f\"[DONE ] {dataset} | {model_name} | seed={seed} \"\n",
    "                f\"| final loss={losses[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "print(\"\\nAll models trained.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution policy\n",
    "\n",
    "The training loop is not executed interactively due to its\n",
    "computational cost.\n",
    "\n",
    "This notebook documents the training procedure used in the paper and is\n",
    "provided for transparency and reproducibility.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (isaac_clean)",
   "language": "python",
   "name": "isaac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
